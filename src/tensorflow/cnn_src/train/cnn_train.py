import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'bo-', label='Train Acc')
    plt.plot(epochs, val_acc, 'ro-', label='Val Acc')
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'bo-', label='Train Loss')
    plt.plot(epochs, val_loss, 'ro-', label='Val Loss')
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig("cnn_train_history.png")
    plt.show()
    print("ğŸ“¸ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: cnn_train_history.png")


# 1. ë°ì´í„° ë¡œë”©
X = np.load("outputs/cnn/X_cnn.npy")
y = np.load("outputs/cnn/y_cnn.npy")

# 2. ë°ì´í„° 3-way ë¶„í•  (Train 70% / Val 20% / Test 10%)

# ë¨¼ì € test 10% ë¶„ë¦¬
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.1, stratify=y, random_state=42
)

# ë‚¨ì€ 90% ì¤‘ì—ì„œ train 70%, val 20% â†’ valì€ 2/9 (ì•½ 22.2%)ë¡œ ë¶„ë¦¬
val_ratio = 2 / 9
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=val_ratio, stratify=y_temp, random_state=42
)

print(f"ğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼:")
print(f"Train: {X_train.shape[0]} / Val: {X_val.shape[0]} / Test: {X_test.shape[0]}")

# 3. CNN ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(14, 130, 1), padding='same'),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# 4. í•™ìŠµ
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=30,
    batch_size=32,
    callbacks=[early_stop]
)

# 5. í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"ğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f} | ì†ì‹¤: {test_loss:.4f}")

# 6. ëª¨ë¸ ì €ì¥
model.save("cnn_model.h5")
# ì‹¤í–‰
plot_history(history)
print("âœ… CNN ëª¨ë¸ ì €ì¥ ì™„ë£Œ: cnn_model.h5")

